{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matt14e/StitchAI/blob/main/Stitchmodel3_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "yDYeNhGI5pIt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qLNwskXOkXEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2540fb8f-d2dc-42d2-f495-7b482d72eb06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install lightning torchvision pyembroidery pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyrao3aZkljC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OT7pe-U1kngl"
      },
      "outputs": [],
      "source": [
        "# Edit here if folders are moved\n",
        "DATA_ROOT = \"/content/drive/MyDrive/Embroidery Files\"\n",
        "IMG_DIR   = f\"{DATA_ROOT}/PNG_image_files\"   # PNGs\n",
        "DST_DIR   = f\"{DATA_ROOT}/DST_digitized_files\"   # DSTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhjS9OvTfT9K"
      },
      "outputs": [],
      "source": [
        "%%writefile dataset.py\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from pyembroidery import read\n",
        "\n",
        "SCALE = 4096.0\n",
        "\n",
        "# --- file patterns -----------------------------------------------------------\n",
        "IMG_EXTS = (\"*.png\", \"*.PNG\", \"*.jpg\", \"*.JPG\")\n",
        "DST_EXTS = (\"*.dst\", \"*.DST\")\n",
        "\n",
        "class EmbroDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Yields (image_tensor, stitch_tensor) pairs.\n",
        "\n",
        "    image_tensor  : C Ã— H Ã— W   float32 in [0, 1]\n",
        "    stitch_tensor : L Ã— 3       (Î”x, Î”y, flag)  float32\n",
        "    \"\"\"\n",
        "    def __init__(self, img_dir, dst_dir, transform=None, max_len=10000):\n",
        "        img_dir, dst_dir = Path(img_dir), Path(dst_dir)\n",
        "\n",
        "        # -------- gather every image and dst file into dicts ---------------\n",
        "        img_files = {}\n",
        "        for pat in IMG_EXTS:\n",
        "            for p in img_dir.glob(pat):\n",
        "                img_files[p.stem] = p\n",
        "\n",
        "        dst_files = {}\n",
        "        for pat in DST_EXTS:\n",
        "            for d in dst_dir.glob(pat):\n",
        "                dst_files[d.stem] = d\n",
        "\n",
        "        # -------- keep only names that exist in *both* dicts --------------\n",
        "        self.common_names = sorted(img_files.keys() & dst_files.keys())\n",
        "        if not self.common_names:\n",
        "            raise RuntimeError(\"No matching (image, DST) pairs found!\")\n",
        "\n",
        "        self.img_files  = img_files\n",
        "        self.dst_files  = dst_files\n",
        "        self.transform  = transform\n",
        "        self.max_len    = max_len\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    def __len__(self):\n",
        "        return len(self.common_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.common_names[idx]\n",
        "\n",
        "        # load (optionally) transform image\n",
        "        img_path = self.img_files[name]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # load DST stitches\n",
        "        dst_path  = self.dst_files[name]\n",
        "\n",
        "        #new\n",
        "        pattern = read(str(dst_path))\n",
        "        raw = torch.tensor(pattern.stitches, dtype=torch.float32)[: self.max_len]\n",
        "        raw[:, :2] /= SCALE\n",
        "        stitches = raw\n",
        "        return img, stitches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAooU3LhlxUf"
      },
      "outputs": [],
      "source": [
        "%%writefile model.py\n",
        "import torch, torch.nn as nn\n",
        "import torch.utils.checkpoint as cp\n",
        "\n",
        "class EmbroNet(nn.Module):\n",
        "    def __init__(self, hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(64,128, 3, 2, 1), nn.ReLU(),\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.enc_fc  = nn.Linear(128, hidden)\n",
        "        self.gru     = nn.GRU(3, hidden, num_layers=2, batch_first=True)\n",
        "        self.dec_fc  = nn.Linear(hidden, 3)\n",
        "\n",
        "    def forward(self, img, prev_cmds):\n",
        "        # Use checkpointing here for encoder layers\n",
        "        def run_encoder(x):\n",
        "            return self.encoder_layers(x)\n",
        "\n",
        "        enc_out = cp.checkpoint(run_encoder, img)  # â† saves memory\n",
        "        pooled = self.pool(enc_out)                # BÃ—128Ã—1Ã—1\n",
        "        h0 = torch.tanh(self.enc_fc(pooled.view(img.size(0), -1))).unsqueeze(0).repeat(2, 1, 1)\n",
        "\n",
        "        #Decoder Checkpointed\n",
        "        def run_decoder(cmds, h0):\n",
        "          return self.gru(cmds, h0)\n",
        "\n",
        "        out, _ = cp.checkpoint(run_decoder, prev_cmds, h0)\n",
        "        return self.dec_fc(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL5UIU_Tlxnz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import lightning as L\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint          # âŠ NEW\n",
        "from lightning.pytorch.loggers   import CSVLogger                # âŠ NEW\n",
        "from dataset import EmbroDataset\n",
        "from model   import EmbroNet\n",
        "\n",
        "# ---------- 1. custom collate_fn ---------------------------------\n",
        "def pad_collate(batch):\n",
        "    \"\"\"\n",
        "    batch = list of (img_tensor, seq_tensor) pairs\n",
        "    Returns:\n",
        "        imgs   : BÃ—3Ã—128Ã—128\n",
        "        tgt    : BÃ—LmaxÃ—3  (padded with 0s)\n",
        "        lens   : list[int] (original sequence lengths)\n",
        "    \"\"\"\n",
        "    imgs, seqs = zip(*batch)\n",
        "    imgs = torch.stack(imgs)                   # all same size\n",
        "\n",
        "    lens    = [s.size(0) for s in seqs]\n",
        "    Lmax    = max(lens)\n",
        "    padded  = torch.zeros(len(seqs), Lmax, 3)  # default zeros = padding\n",
        "    for i, s in enumerate(seqs):\n",
        "        padded[i, : lens[i], :] = s\n",
        "    return imgs, padded, torch.tensor(lens)\n",
        "\n",
        "# ---------- 2. LightningModule -----------------------------------\n",
        "class LitModule(L.LightningModule):\n",
        "    def __init__(self, ss_start=1.0, ss_end=0.0, ss_epochs=40):\n",
        "        super().__init__()\n",
        "        self.net = EmbroNet()\n",
        "        self.loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
        "        self.ss_start = ss_start\n",
        "        self.ss_end = ss_end\n",
        "        self.ss_epochs = ss_epochs\n",
        "\n",
        "    def current_ss_ratio(self):\n",
        "        # Linearly decay teacher-forcing ratio over ss_epochs\n",
        "        epoch = self.current_epoch\n",
        "        ratio = self.ss_start - (self.ss_start - self.ss_end) * min(epoch, self.ss_epochs) / self.ss_epochs\n",
        "        return ratio\n",
        "\n",
        "    def forward(self, img, prev_cmds):\n",
        "        return self.net(img, prev_cmds)\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        img, tgt, lens = batch  # tgt: BÃ—LmaxÃ—3\n",
        "        B, Lmax, _ = tgt.shape\n",
        "        ss_ratio = self.current_ss_ratio()\n",
        "        print(\"ğŸ’¡ Input shape:\", img.shape)  # [B, 3, H, W]\n",
        "        print(\"ğŸ’¡ Target shape:\", tgt.shape)  # [B, Lmax, 3]\n",
        "        print(\"ğŸ’¡ Max seq length:\", max(lens).item())\n",
        "\n",
        "\n",
        "        # Start with BOS (first stitch)\n",
        "        preds = []\n",
        "        prev = tgt[:, :1]  # BÃ—1Ã—3\n",
        "\n",
        "        for t in range(1, Lmax):\n",
        "            out = self(img, prev)  # BÃ—tÃ—3 â†’ predict next\n",
        "            pred_next = out[:, -1:]  # BÃ—1Ã—3\n",
        "            preds.append(pred_next)\n",
        "\n",
        "            # scheduled sampling: feed either prediction or ground truth\n",
        "            use_teacher = (torch.rand(B, 1, 1, device=self.device) < ss_ratio)\n",
        "            next_input = torch.where(use_teacher, tgt[:, t:t+1], pred_next.detach())\n",
        "            #prev = torch.cat([prev, next_input], dim=1)  # grow the sequence\n",
        "            prev = torch.cat([prev, next_input.detach()], dim=1)\n",
        "\n",
        "\n",
        "        # build full prediction tensor\n",
        "        pred_seq = torch.cat(preds, dim=1)  # BÃ—(Lmaxâˆ’1)Ã—3\n",
        "\n",
        "        # Mask-aware loss\n",
        "        loss = 0.0\n",
        "        for i, L in enumerate(lens):\n",
        "            L = L.item()\n",
        "            loss += self.loss_fn(pred_seq[i, :L-1], tgt[i, 1:L])\n",
        "        loss = loss / len(lens)\n",
        "\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"ss_ratio\", ss_ratio, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.AdamW(self.parameters(), 1e-3)\n",
        "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5, factor=0.5)\n",
        "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"monitor\": \"train_loss\"}}\n",
        "\n",
        "\n",
        "# ---------- 3. dataset & dataloader ------------------------------\n",
        "####OLD====tfms = Compose([Resize((128,128)), ToTensor()])\n",
        "####NEW####\n",
        "\n",
        "from PIL import ImageOps\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Custom transform function\n",
        "def custom_resize_pad_to_tensor(img, target_size=256):\n",
        "    # Resize shortest side to target_size\n",
        "    if img.width < img.height:\n",
        "        new_w = target_size\n",
        "        new_h = int(img.height * (target_size / img.width))\n",
        "    else:\n",
        "        new_h = target_size\n",
        "        new_w = int(img.width * (target_size / img.height))\n",
        "    img = img.resize((new_w, new_h), Image.LANCZOS)\n",
        "\n",
        "    # Add white padding to center image in square\n",
        "    pad_w = target_size - img.width if img.width < target_size else 0\n",
        "    pad_h = target_size - img.height if img.height < target_size else 0\n",
        "    left = pad_w // 2\n",
        "    right = pad_w - left\n",
        "    top = pad_h // 2\n",
        "    bottom = pad_h - top\n",
        "    img = ImageOps.expand(img, border=(left, top, right, bottom), fill=(255, 255, 255))\n",
        "\n",
        "    # Ensure final size is exact\n",
        "    img = img.resize((target_size, target_size))\n",
        "\n",
        "    return ToTensor()(img)\n",
        "\n",
        "# Set your transform\n",
        "tfms = custom_resize_pad_to_tensor\n",
        "\n",
        "# Now create dataset\n",
        "ds = EmbroDataset(IMG_DIR, DST_DIR, tfms)\n",
        "\n",
        "dl = DataLoader(ds,\n",
        "                batch_size=2,\n",
        "                shuffle=True,\n",
        "                num_workers=0,\n",
        "                collate_fn=pad_collate)\n",
        "\n",
        "# ---------- 4. train ---------------------------------------------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/Embroidery Files/checkpoints3\"   # â‹ choose your folder\n",
        "LOG_DIR  = \"/content/drive/MyDrive/Embroidery Files/logs3\"          # â‹ choose your folder\n",
        "\n",
        "ckpt_cb = ModelCheckpoint(\n",
        "    dirpath   = CKPT_DIR,\n",
        "    filename  = \"epoch{epoch:02d}-loss{train_loss:.3f}\",\n",
        "    save_last = True,           # keeps 'last.ckpt' every epoch\n",
        "    save_top_k= 3,              # best 3 by train_loss\n",
        "    monitor   = \"train_loss\",\n",
        "    mode      = \"min\",\n",
        ")\n",
        "\n",
        "csv_logger = CSVLogger(save_dir=LOG_DIR, name=\"run1\")\n",
        "\n",
        "#trainer = L.Trainer(fast_dev_run=True)\n",
        "trainer = L.Trainer(\n",
        "    max_epochs        = 16,     # or however many you want\n",
        "    accelerator       = \"gpu\",  # switch Colab to GPU runtime!\n",
        "    precision         = \"16-mixed\",\n",
        "    callbacks         = [ckpt_cb],\n",
        "    logger            = csv_logger,\n",
        "    log_every_n_steps = 10,\n",
        ")\n",
        "####Verify####\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "# View one sample from the dataset\n",
        "img_tensor, _ = ds[0]\n",
        "img = ToPILImage()(img_tensor)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.title(\"Sample Transformed Image (From Training Data)\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#Train\n",
        "\n",
        "trainer.fit(LitModule(), dl)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
      ],
      "metadata": {
        "id": "6h9fWA2rYnR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "# ğŸ” 1.  find the most recent metrics.csv anywhere in Drive or /content\n",
        "candidates = glob.glob(\"/content/**/metrics.csv\", recursive=True) + \\\n",
        "             glob.glob(\"/content/drive/**/metrics.csv\", recursive=True)\n",
        "\n",
        "if not candidates:\n",
        "    raise FileNotFoundError(\"No metrics.csv files foundâ€”did your run finish an epoch and use CSVLogger?\")\n",
        "\n",
        "csv_path = max(candidates, key=os.path.getmtime)   # newest one\n",
        "print(\"Using:\", csv_path)\n",
        "\n",
        "# ğŸ“Š 2.  read and plot epoch-level loss\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# pick the epoch-averaged column\n",
        "loss_col = \"train_loss_epoch\" if \"train_loss_epoch\" in df.columns else None\n",
        "if loss_col is None:\n",
        "    print(\"No epoch-level loss in file; did you train less than 1 epoch?\")\n",
        "else:\n",
        "    plt.plot(df[loss_col].dropna().values)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.title(\"epoch-average train_loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "e_DwthA-ry2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "# locate the latest metrics.csv\n",
        "csv = max(glob.glob(\"/content/**/metrics.csv\", recursive=True) +\n",
        "          glob.glob(\"/content/drive/**/metrics.csv\", recursive=True),\n",
        "          key=os.path.getmtime)\n",
        "\n",
        "df = pd.read_csv(csv)\n",
        "\n",
        "if 'epoch' not in df.columns or 'train_loss_step' not in df.columns:\n",
        "    print(\"File lacks epoch or train_loss_step columns.\")\n",
        "else:\n",
        "    epoch_mean = df.groupby('epoch')['train_loss_step'].mean()\n",
        "    print(epoch_mean.tail())          # show last few\n",
        "    plt.plot(epoch_mean.values)\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('avg train_loss_step')\n",
        "    plt.title('epoch-level loss (computed)')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "6ry_RyTrt8tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after your training run finishes\n",
        "trainer.save_checkpoint(\"/content/drive/MyDrive/Embroidery Files/embronet_latest.ckpt\")\n"
      ],
      "metadata": {
        "id": "elEXvBfIi7-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## (A) List any checkpoints Lightning has already saved\n",
        "import glob, pprint\n",
        "ckpts = glob.glob(\"/content/**/checkpoints/*.ckpt\", recursive=True)\n",
        "ckpts += glob.glob(\"/content/drive/**/checkpoints/*.ckpt\", recursive=True)\n",
        "pprint.pp(ckpts)\n",
        "\n",
        "# If the list prints something, copy one path:\n",
        "# CKPT_PATH = \"/content/lightning_logs/version_3/checkpoints/epoch=4-step=99.ckpt\"\n",
        "\n",
        "# (B) If the list is empty, save a quick checkpoint now (after training):\n",
        "#trainer.save_checkpoint(\"/content/drive/MyDrive/EmbroideryTests/embronet.ckpt\")\n",
        "#CKPT_PATH = \"/content/drive/MyDrive/EmbroideryTests/embronet.ckpt\"\n"
      ],
      "metadata": {
        "id": "GZd3T3lKqMga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, pprint, os\n",
        "\n",
        "# search inside the folder where you asked ModelCheckpoint to save\n",
        "ckpts = glob.glob(\"/content/drive/MyDrive/Embroidery Files/checkpoints/*.ckpt\")\n",
        "pprint.pp(ckpts)\n"
      ],
      "metadata": {
        "id": "Ox8NsmF0vfxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CKPT_PATH = \"/content/drive/MyDrive/Embroidery Files/checkpoints/last.ckpt\""
      ],
      "metadata": {
        "id": "SVfgY9vp3st3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"checkpoint exists?\", os.path.exists(CKPT_PATH))"
      ],
      "metadata": {
        "id": "165c0pq3ZEPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are starting again. Here is it\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PI34AIIeCrab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyPI installs (run once per session)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip -q install pyembroidery pillow\n"
      ],
      "metadata": {
        "id": "efRlC0QkCy5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Paths + Global Constants #######\n",
        "\n",
        "# --------- EDIT THESE THREE PATHS ---------------------------------\n",
        "PNG_PATH   = \"/content/drive/MyDrive/Embroidery Files/pngstart/Copy of Cornerstone_Logo1.PNG\"\n",
        "CKPT_PATH  = \"/content/drive/MyDrive/Embroidery Files/checkpoints/last-v2.ckpt\"\n",
        "OUT_PATH   = \"/content/drive/MyDrive/Embroidery Files/outputs/test_logo15.dst\"\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "SCALE_TRAIN = 4096.0   # the divisor you added in dataset.py\n",
        "DESIGN_MM   = 100.0    # longest finished side (mm)\n",
        "MAX_LEN     = 4096     # safety cap on generated stitches\n"
      ],
      "metadata": {
        "id": "16WVJde7esTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Load Model + Run Inference ######\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from model import EmbroNet          # make sure model.py exists in /content\n",
        "\n",
        "# 1) Load network\n",
        "net = EmbroNet().eval()\n",
        "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "state = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
        "state = {k.replace(\"net.\", \"\"): v for k, v in state.items()}\n",
        "net.load_state_dict(state, strict=False)\n",
        "print(\"âœ“ checkpoint loaded\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2) Image â†’ tensor\n",
        "tfms = Compose([Resize((128,128)), ToTensor()])\n",
        "img = tfms(Image.open(PNG_PATH).convert(\"RGB\")).unsqueeze(0)\n",
        "\n",
        "# 3) Autoregressive generate\n",
        "seq = torch.zeros(1,1,3)\n",
        "with torch.no_grad():\n",
        "    for _ in range(MAX_LEN):\n",
        "        seq = torch.cat([seq, net(img, seq)[:, -1:, :]], 1)\n",
        "\n",
        "pred_seq = seq.squeeze(0)[1:]      # LÃ—3  (Î”x,Î”y,flag)\n"
      ],
      "metadata": {
        "id": "PD_vP2PTfi8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================  CELL 4  ====================\n",
        "import numpy as np, pathlib\n",
        "from pyembroidery import EmbPattern, write\n",
        "\n",
        "# --- 1. undo train-time shrink (pixels) ---------------------------\n",
        "pred_pixels = pred_seq.clone()\n",
        "pred_pixels[:, :2] *= SCALE_TRAIN            # back to pixel deltas\n",
        "\n",
        "# --- 2. pixels â†’ millimetres, fit DESIGN_MM ----------------------\n",
        "xy_pix = pred_pixels[:, :2].cumsum(0)\n",
        "span_pix = (xy_pix.max(0).values - xy_pix.min(0).values).max()\n",
        "px_to_mm = DESIGN_MM / span_pix.item()\n",
        "\n",
        "xy_mm_abs = xy_pix * px_to_mm                # absolute mm coords\n",
        "xy_mm_abs -= xy_mm_abs.min(0).values         # shift to (0,0)\n",
        "\n",
        "print(\"bbox in mm:\", np.ptp(xy_mm_abs.numpy(), axis=0))  # expect ~[ 55.9 100 ]\n",
        "\n",
        "# --- 3. build EmbPattern & save (mm â†’ 0.1-mm inside pyembroidery) -\n",
        "pat = EmbPattern()\n",
        "deltas_mm = np.diff(                                # first row = (0,0)\n",
        "    np.vstack([np.zeros((1,2)), xy_mm_abs.numpy()]),\n",
        "    axis=0,\n",
        ")\n",
        "for dx_mm, dy_mm in deltas_mm:                     # relative mm moves âœ…\n",
        "    pat.stitch(float(dx_mm * 10), float(dy_mm * 10))\n",
        "pat.end()\n",
        "\n",
        "pathlib.Path(OUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "write(pat, OUT_PATH)\n",
        "print(\"âœ“ saved\", OUT_PATH)\n",
        "\n",
        "# --- D. sanity print (correct) -----------------------------------\n",
        "import numpy as np\n",
        "from pyembroidery import read\n",
        "\n",
        "pat = read(OUT_PATH)\n",
        "coords = np.array([(x, y) for x, y, _ in pat.stitches])  # absolute, already 0.1-mm\n",
        "\n",
        "span_units = np.ptp(coords, axis=0)          # max âˆ’ min in 0.1-mm units\n",
        "print(\"Design final size: {:.1f} Ã— {:.1f} mm\"\n",
        "      .format(span_units[0] / 10, span_units[1] / 10))\n"
      ],
      "metadata": {
        "id": "2jwC6WqYxSvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOOLS"
      ],
      "metadata": {
        "id": "ilJ-wROJP_qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####Stitch Count#####\n",
        "\n",
        "from pyembroidery import read\n",
        "\n",
        "pattern = read(\"/content/drive/MyDrive/Embroidery Files/outputs/test_logo15.dst\")\n",
        "print(f\"Total stitches: {len(pattern.stitches)}\")"
      ],
      "metadata": {
        "id": "7PfBzOQejTiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####Checking the checkpoints######\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Replace with your checkpoint path\n",
        "ckpt_path = \"/content/drive/MyDrive/Embroidery Files/checkpoints/last-v1.ckpt\"\n",
        "\n",
        "# Get the last modified time\n",
        "timestamp = os.path.getmtime(ckpt_path)\n",
        "print(\"ğŸ•’ Checkpoint last modified at:\", datetime.fromtimestamp(timestamp))\n"
      ],
      "metadata": {
        "id": "9yR_cFP5KOlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_padding(img, target_size):\n",
        "    w, h = img.size\n",
        "    pad_w = target_size - w if w < target_size else 0\n",
        "    pad_h = target_size - h if h < target_size else 0\n",
        "    left = pad_w // 2\n",
        "    right = pad_w - left\n",
        "    top = pad_h // 2\n",
        "    bottom = pad_h - top\n",
        "    return (left, top, right, bottom)\n",
        "\n",
        "img = Image.open(\"/content/drive/MyDrive/Embroidery Files/PNG_image_files/Cornerstone_Logo.PNG\").convert(\"RGB\")\n",
        "img.thumbnail((128, 128), Image.LANCZOS)\n",
        "padded = ImageOps.expand(img, get_padding(img, 128), fill=(255, 255, 255))\n",
        "\n",
        "plt.imshow(padded)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"256x256 with White Padding\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n7FWARFNQMQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Padding function â€” already in your codebase\n",
        "def get_padding(img, target_size):\n",
        "    w, h = img.size\n",
        "    pad_w = target_size - w if w < target_size else 0\n",
        "    pad_h = target_size - h if h < target_size else 0\n",
        "    left   = pad_w // 2\n",
        "    right  = pad_w - left\n",
        "    top    = pad_h // 2\n",
        "    bottom = pad_h - top\n",
        "    return (left, top, right, bottom)\n",
        "\n",
        "# Load and process image (your PNG)\n",
        "img_path = \"/content/drive/MyDrive/Embroidery Files/PNG_image_files/Cornerstone_Logo.PNG\"\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "# Resize shortest side to TARGET_SIZE while preserving aspect ratio\n",
        "TARGET_SIZE = 256\n",
        "img.thumbnail((TARGET_SIZE, TARGET_SIZE), Image.LANCZOS)\n",
        "\n",
        "# Add white padding\n",
        "padded = ImageOps.expand(img, get_padding(img, TARGET_SIZE), fill=(255, 255, 255))\n",
        "\n",
        "# Optional: enforce final 256x256 size (sometimes rounding causes 255x256)\n",
        "final = padded.resize((TARGET_SIZE, TARGET_SIZE), Image.LANCZOS)\n",
        "\n",
        "# Show preview\n",
        "plt.imshow(final)\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Preview: {TARGET_SIZE}x{TARGET_SIZE} (White Padding)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kEEqbjCaYpRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO/9uhwZ3rUAYLPdAZSZXjD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matt14e/StitchAI/blob/main/Stitchmodel3_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLNwskXOkXEr",
        "outputId": "7cc363d3-cc21-4453-e42a-bacc86e7fd33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install lightning torchvision pyembroidery pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyrao3aZkljC",
        "outputId": "9136b791-2355-4541-94e4-7a0ce06c8933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT7pe-U1kngl"
      },
      "outputs": [],
      "source": [
        "# Edit here if folders are moved\n",
        "DATA_ROOT = \"/content/drive/MyDrive/Embroidery Files\"\n",
        "IMG_DIR   = f\"{DATA_ROOT}/PNG_image_files\"   # PNGs\n",
        "DST_DIR   = f\"{DATA_ROOT}/DST_digitized_files\"   # DSTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hN6wh5IktHu",
        "outputId": "3577896b-73b4-48ae-ad87-e7ce477f3aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“·  sample PNG files:\n",
            "    Cod_Fish_Logo.PNG\n",
            "    MLPP_LOGO.PNG\n",
            "    LF_LOGO.PNG\n",
            "    Andrews_Logo.PNG\n",
            "    Royal_Logo.PNG\n",
            "\n",
            "ğŸ§µ  sample DST files:\n",
            "    Elite.DST\n",
            "    adam.DST\n",
            "    Lehigh.DST\n",
            "    Moon-Walkaz-Text.DST\n",
            "    Kamp.DST\n"
          ]
        }
      ],
      "source": [
        "#sanity check the folders\n",
        "import pathlib, textwrap\n",
        "\n",
        "print(\"ğŸ“·  sample PNG files:\")\n",
        "for p in list(pathlib.Path(IMG_DIR).glob(\"*.[pP][nN][gG]\"))[:5]:\n",
        "    print(\"   \", p.name)\n",
        "\n",
        "print(\"\\nğŸ§µ  sample DST files:\")\n",
        "for p in list(pathlib.Path(DST_DIR).glob(\"*.[dD][sS][tT]\"))[:5]:\n",
        "    print(\"   \", p.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhjS9OvTfT9K",
        "outputId": "e9df3c16-a058-4b74-e925-5ba9b6259a85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing dataset.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile dataset.py\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from pyembroidery import read\n",
        "\n",
        "# --- file patterns -----------------------------------------------------------\n",
        "IMG_EXTS = (\"*.png\", \"*.PNG\", \"*.jpg\", \"*.JPG\")\n",
        "DST_EXTS = (\"*.dst\", \"*.DST\")\n",
        "\n",
        "class EmbroDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Yields (image_tensor, stitch_tensor) pairs.\n",
        "\n",
        "    image_tensor  : C Ã— H Ã— W   float32 in [0, 1]\n",
        "    stitch_tensor : L Ã— 3       (Î”x, Î”y, flag)  float32\n",
        "    \"\"\"\n",
        "    def __init__(self, img_dir, dst_dir, transform=None, max_len=4096):\n",
        "        img_dir, dst_dir = Path(img_dir), Path(dst_dir)\n",
        "\n",
        "        # -------- gather every image and dst file into dicts ---------------\n",
        "        img_files = {}\n",
        "        for pat in IMG_EXTS:\n",
        "            for p in img_dir.glob(pat):\n",
        "                img_files[p.stem] = p\n",
        "\n",
        "        dst_files = {}\n",
        "        for pat in DST_EXTS:\n",
        "            for d in dst_dir.glob(pat):\n",
        "                dst_files[d.stem] = d\n",
        "\n",
        "        # -------- keep only names that exist in *both* dicts --------------\n",
        "        self.common_names = sorted(img_files.keys() & dst_files.keys())\n",
        "        if not self.common_names:\n",
        "            raise RuntimeError(\"No matching (image, DST) pairs found!\")\n",
        "\n",
        "        self.img_files  = img_files\n",
        "        self.dst_files  = dst_files\n",
        "        self.transform  = transform\n",
        "        self.max_len    = max_len\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    def __len__(self):\n",
        "        return len(self.common_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.common_names[idx]\n",
        "\n",
        "        # load & optionally transform image\n",
        "        img_path = self.img_files[name]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # load DST stitches\n",
        "        dst_path  = self.dst_files[name]\n",
        "        pattern   = read(str(dst_path))\n",
        "        stitches  = torch.tensor(pattern.stitches,\n",
        "                                 dtype=torch.float32)[: self.max_len]\n",
        "\n",
        "        return img, stitches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyfHUvedlvK_",
        "outputId": "2ecc9fcc-8352-46d1-a264-8cd323c65870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting dataset.py\n"
          ]
        }
      ],
      "source": [
        "#dont run\n",
        "\n",
        "%%writefile dataset.py\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from pyembroidery import read\n",
        "\n",
        "IMG_EXTS = (\"*.png\", \"*.PNG\", \"*.jpg\", \"*.JPG\")\n",
        "DST_EXTS = (\"*.dst\", \"*.DST\")\n",
        "\n",
        "class EmbroDataset(Dataset):\n",
        "    \"\"\"\n",
        "    (image_tensor, stitch_tensor) pairs\n",
        "    image_tensor  : CÃ—HÃ—W  float32 [0,1]\n",
        "    stitch_tensor : LÃ—3    (Î”x, Î”y, flag)\n",
        "    \"\"\"\n",
        "    def __init__(self, img_dir, dst_dir, transform=None, max_len=4096):\n",
        "        self.img_paths = []\n",
        "        for pat in IMG_EXTS:\n",
        "            self.img_paths.extend(Path(img_dir).glob(pat))\n",
        "        self.img_paths = sorted(self.img_paths)\n",
        "\n",
        "        self.dst_dir   = Path(dst_dir)\n",
        "        self.transform = transform\n",
        "        self.max_len   = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        stem = img_path.stem\n",
        "        for ext in DST_EXTS:\n",
        "            cand = self.dst_dir / f\"{stem}{ext[1:]}\"   # '*.dst' âœ '.dst'\n",
        "            if cand.exists():\n",
        "                dst_path = cand\n",
        "                break\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"No DST match for {stem}\")\n",
        "\n",
        "        pattern  = read(str(dst_path))\n",
        "        stitches = torch.tensor(pattern.stitches,\n",
        "                                dtype=torch.float32)[: self.max_len]\n",
        "        return img, stitches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAooU3LhlxUf",
        "outputId": "c2278c46-ead8-4cfd-b224-37aeb24f88f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "import torch, torch.nn as nn\n",
        "\n",
        "class EmbroNet(nn.Module):\n",
        "    \"\"\"Tiny CNN encoder + GRU decoder baseline.\"\"\"\n",
        "    def __init__(self, hidden=256):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(64,128, 3, 2, 1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1)          # 128Ã—1Ã—1\n",
        "        )\n",
        "        self.enc_fc  = nn.Linear(128, hidden)\n",
        "\n",
        "        self.gru     = nn.GRU(3, hidden, num_layers=2, batch_first=True)\n",
        "        self.dec_fc  = nn.Linear(hidden, 3)\n",
        "\n",
        "    def forward(self, img, prev_cmds):\n",
        "        B = img.size(0)\n",
        "        h0 = self.encoder(img).view(B, -1)      # BÃ—128\n",
        "        h0 = torch.tanh(self.enc_fc(h0)).unsqueeze(0).repeat(2, 1, 1)\n",
        "        out, _ = self.gru(prev_cmds, h0)        # BÃ—LÃ—H\n",
        "        return self.dec_fc(out)                 # BÃ—LÃ—3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730,
          "referenced_widgets": [
            "210cb5d424d04ec894bb28b21841bc59",
            "210ef896db1d4bc1957423bd28637835",
            "fd09fec865aa484ca7b3ca13069b6663",
            "969e6f39ccbb4bb39ad712d69ad86694",
            "9d83f7a12e514e7bb564fbca4b8e5c0c",
            "487ecb4c6c27462b9c48e3839d6c72e4",
            "e339ef57905e4c349b06318884c0bb47",
            "98e0d61365c54b8a861b852f9d1fb0e4",
            "142d6798c6f84167be1ca464f11a9c74",
            "a6dcb026b493439995ef961079f9b028",
            "3c18cb04b5034eebb3aa803d9e4212da"
          ]
        },
        "id": "uL5UIU_Tlxnz",
        "outputId": "9e716953-84e7-4ff4-e9c7-ab8d57539e78"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
            "INFO: Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length: 162\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: \n",
            "  | Name    | Type     | Params | Mode \n",
            "---------------------------------------------\n",
            "0 | net     | EmbroNet | 722 K  | train\n",
            "1 | loss_fn | MSELoss  | 0      | train\n",
            "---------------------------------------------\n",
            "722 K     Trainable params\n",
            "0         Non-trainable params\n",
            "722 K     Total params\n",
            "2.889     Total estimated model params size (MB)\n",
            "13        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name    | Type     | Params | Mode \n",
            "---------------------------------------------\n",
            "0 | net     | EmbroNet | 722 K  | train\n",
            "1 | loss_fn | MSELoss  | 0      | train\n",
            "---------------------------------------------\n",
            "722 K     Trainable params\n",
            "0         Non-trainable params\n",
            "722 K     Total params\n",
            "2.889     Total estimated model params size (MB)\n",
            "13        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "210cb5d424d04ec894bb28b21841bc59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import lightning as L\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from dataset import EmbroDataset\n",
        "from model   import EmbroNet\n",
        "\n",
        "# ---------- 1. custom collate_fn ---------------------------------\n",
        "def pad_collate(batch):\n",
        "    \"\"\"\n",
        "    batch = list of (img_tensor, seq_tensor) pairs\n",
        "    Returns:\n",
        "        imgs   : BÃ—3Ã—128Ã—128\n",
        "        tgt    : BÃ—LmaxÃ—3  (padded with 0s)\n",
        "        lens   : list[int] (original sequence lengths)\n",
        "    \"\"\"\n",
        "    imgs, seqs = zip(*batch)\n",
        "    imgs = torch.stack(imgs)                   # all same size\n",
        "\n",
        "    lens    = [s.size(0) for s in seqs]\n",
        "    Lmax    = max(lens)\n",
        "    padded  = torch.zeros(len(seqs), Lmax, 3)  # default zeros = padding\n",
        "    for i, s in enumerate(seqs):\n",
        "        padded[i, : lens[i], :] = s\n",
        "    return imgs, padded, torch.tensor(lens)\n",
        "\n",
        "# ---------- 2. LightningModule -----------------------------------\n",
        "class LitModule(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = EmbroNet()\n",
        "        self.loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
        "\n",
        "    def forward(self, img, prev_cmds):\n",
        "        return self.net(img, prev_cmds)\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        img, tgt, lens = batch                 # tgt: BÃ—LmaxÃ—3\n",
        "        pred = self(img, tgt[:, :-1])\n",
        "\n",
        "        # compute loss mask-aware\n",
        "        loss = 0.0\n",
        "        for i, L in enumerate(lens):\n",
        "            loss += self.loss_fn(pred[i, : L-1], tgt[i, 1 : L])  # valid region\n",
        "        loss = loss / len(lens)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), 1e-3)\n",
        "\n",
        "# ---------- 3. dataset & dataloader ------------------------------\n",
        "tfms = Compose([Resize((128,128)), ToTensor()])\n",
        "ds   = EmbroDataset(IMG_DIR, DST_DIR, tfms)\n",
        "print(\"Dataset length:\", len(ds))          # should be > 0\n",
        "\n",
        "dl   = DataLoader(ds,\n",
        "                  batch_size=8,\n",
        "                  shuffle=True,\n",
        "                  num_workers=0,            # change later if you like\n",
        "                  collate_fn=pad_collate)   # â† custom collate!\n",
        "\n",
        "# ---------- 4. train ---------------------------------------------\n",
        "trainer = L.Trainer(max_epochs=10,\n",
        "                    precision=\"16-mixed\",\n",
        "                    accelerator=\"auto\")\n",
        "trainer.fit(LitModule(), dl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTVlEDlYg75Y"
      },
      "outputs": [],
      "source": [
        "img, tgt = next(iter(dl))\n",
        "net = trainer.model.net.to(img.device)\n",
        "with torch.no_grad():\n",
        "    pred = net(img, tgt[:, :-1])\n",
        "print(\"GT  :\", tgt[0, :5])\n",
        "print(\"Pred:\", pred[0, :5])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5zYBhdB1OIhMjJsmdjZ7C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
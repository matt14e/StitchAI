{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5zYBhdB1OIhMjJsmdjZ7C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"210cb5d424d04ec894bb28b21841bc59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_210ef896db1d4bc1957423bd28637835","IPY_MODEL_fd09fec865aa484ca7b3ca13069b6663","IPY_MODEL_969e6f39ccbb4bb39ad712d69ad86694"],"layout":"IPY_MODEL_9d83f7a12e514e7bb564fbca4b8e5c0c"}},"210ef896db1d4bc1957423bd28637835":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_487ecb4c6c27462b9c48e3839d6c72e4","placeholder":"‚Äã","style":"IPY_MODEL_e339ef57905e4c349b06318884c0bb47","value":"Epoch‚Äá0:‚Äá‚Äá‚Äá0%"}},"fd09fec865aa484ca7b3ca13069b6663":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_98e0d61365c54b8a861b852f9d1fb0e4","max":21,"min":0,"orientation":"horizontal","style":"IPY_MODEL_142d6798c6f84167be1ca464f11a9c74","value":0}},"969e6f39ccbb4bb39ad712d69ad86694":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6dcb026b493439995ef961079f9b028","placeholder":"‚Äã","style":"IPY_MODEL_3c18cb04b5034eebb3aa803d9e4212da","value":"‚Äá0/21‚Äá[00:00&lt;?,‚Äá?it/s]"}},"9d83f7a12e514e7bb564fbca4b8e5c0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"487ecb4c6c27462b9c48e3839d6c72e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e339ef57905e4c349b06318884c0bb47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98e0d61365c54b8a861b852f9d1fb0e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"142d6798c6f84167be1ca464f11a9c74":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6dcb026b493439995ef961079f9b028":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c18cb04b5034eebb3aa803d9e4212da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLNwskXOkXEr","executionInfo":{"status":"ok","timestamp":1746026713432,"user_tz":360,"elapsed":127816,"user":{"displayName":"Matt Elmer","userId":"02352746252018223529"}},"outputId":"7cc363d3-cc21-4453-e42a-bacc86e7fd33"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install lightning torchvision pyembroidery pillow"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyrao3aZkljC","executionInfo":{"status":"ok","timestamp":1746028039283,"user_tz":360,"elapsed":7996,"user":{"displayName":"Matt Elmer","userId":"02352746252018223529"}},"outputId":"9136b791-2355-4541-94e4-7a0ce06c8933"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Edit here if folders are moved\n","DATA_ROOT = \"/content/drive/MyDrive/Embroidery Files\"\n","IMG_DIR   = f\"{DATA_ROOT}/PNG_image_files\"   # PNGs\n","DST_DIR   = f\"{DATA_ROOT}/DST_digitized_files\"   # DSTs"],"metadata":{"id":"OT7pe-U1kngl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#sanity check the folders\n","import pathlib, textwrap\n","\n","print(\"üì∑  sample PNG files:\")\n","for p in list(pathlib.Path(IMG_DIR).glob(\"*.[pP][nN][gG]\"))[:5]:\n","    print(\"   \", p.name)\n","\n","print(\"\\nüßµ  sample DST files:\")\n","for p in list(pathlib.Path(DST_DIR).glob(\"*.[dD][sS][tT]\"))[:5]:\n","    print(\"   \", p.name)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2hN6wh5IktHu","executionInfo":{"status":"ok","timestamp":1746028046376,"user_tz":360,"elapsed":2071,"user":{"displayName":"Matt Elmer","userId":"02352746252018223529"}},"outputId":"3577896b-73b4-48ae-ad87-e7ce477f3aa1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üì∑  sample PNG files:\n","    Cod_Fish_Logo.PNG\n","    MLPP_LOGO.PNG\n","    LF_LOGO.PNG\n","    Andrews_Logo.PNG\n","    Royal_Logo.PNG\n","\n","üßµ  sample DST files:\n","    Elite.DST\n","    adam.DST\n","    Lehigh.DST\n","    Moon-Walkaz-Text.DST\n","    Kamp.DST\n"]}]},{"cell_type":"code","source":["%%writefile dataset.py\n","from pathlib import Path\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","from pyembroidery import read\n","\n","# --- file patterns -----------------------------------------------------------\n","IMG_EXTS = (\"*.png\", \"*.PNG\", \"*.jpg\", \"*.JPG\")\n","DST_EXTS = (\"*.dst\", \"*.DST\")\n","\n","class EmbroDataset(Dataset):\n","    \"\"\"\n","    Yields (image_tensor, stitch_tensor) pairs.\n","\n","    image_tensor  : C √ó H √ó W   float32 in [0, 1]\n","    stitch_tensor : L √ó 3       (Œîx, Œîy, flag)  float32\n","    \"\"\"\n","    def __init__(self, img_dir, dst_dir, transform=None, max_len=4096):\n","        img_dir, dst_dir = Path(img_dir), Path(dst_dir)\n","\n","        # -------- gather every image and dst file into dicts ---------------\n","        img_files = {}\n","        for pat in IMG_EXTS:\n","            for p in img_dir.glob(pat):\n","                img_files[p.stem] = p\n","\n","        dst_files = {}\n","        for pat in DST_EXTS:\n","            for d in dst_dir.glob(pat):\n","                dst_files[d.stem] = d\n","\n","        # -------- keep only names that exist in *both* dicts --------------\n","        self.common_names = sorted(img_files.keys() & dst_files.keys())\n","        if not self.common_names:\n","            raise RuntimeError(\"No matching (image, DST) pairs found!\")\n","\n","        self.img_files  = img_files\n","        self.dst_files  = dst_files\n","        self.transform  = transform\n","        self.max_len    = max_len\n","\n","    # ------------------------------------------------------------------------\n","    def __len__(self):\n","        return len(self.common_names)\n","\n","    def __getitem__(self, idx):\n","        name = self.common_names[idx]\n","\n","        # load & optionally transform image\n","        img_path = self.img_files[name]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        # load DST stitches\n","        dst_path  = self.dst_files[name]\n","        pattern   = read(str(dst_path))\n","        stitches  = torch.tensor(pattern.stitches,\n","                                 dtype=torch.float32)[: self.max_len]\n","\n","        return img, stitches\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhjS9OvTfT9K","executionInfo":{"status":"ok","timestamp":1746028048903,"user_tz":360,"elapsed":18,"user":{"displayName":"Matt Elmer","userId":"02352746252018223529"}},"outputId":"e9df3c16-a058-4b74-e925-5ba9b6259a85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing dataset.py\n"]}]},{"cell_type":"code","source":["#dont run\n","\n","%%writefile dataset.py\n","from pathlib import Path\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","from pyembroidery import read\n","\n","IMG_EXTS = (\"*.png\", \"*.PNG\", \"*.jpg\", \"*.JPG\")\n","DST_EXTS = (\"*.dst\", \"*.DST\")\n","\n","class EmbroDataset(Dataset):\n","    \"\"\"\n","    (image_tensor, stitch_tensor) pairs\n","    image_tensor  : C√óH√óW  float32 [0,1]\n","    stitch_tensor : L√ó3    (Œîx, Œîy, flag)\n","    \"\"\"\n","    def __init__(self, img_dir, dst_dir, transform=None, max_len=4096):\n","        self.img_paths = []\n","        for pat in IMG_EXTS:\n","            self.img_paths.extend(Path(img_dir).glob(pat))\n","        self.img_paths = sorted(self.img_paths)\n","\n","        self.dst_dir   = Path(dst_dir)\n","        self.transform = transform\n","        self.max_len   = max_len\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        stem = img_path.stem\n","        for ext in DST_EXTS:\n","            cand = self.dst_dir / f\"{stem}{ext[1:]}\"   # '*.dst' ‚ûú '.dst'\n","            if cand.exists():\n","                dst_path = cand\n","                break\n","        else:\n","            raise FileNotFoundError(f\"No DST match for {stem}\")\n","\n","        pattern  = read(str(dst_path))\n","        stitches = torch.tensor(pattern.stitches,\n","                                dtype=torch.float32)[: self.max_len]\n","        return img, stitches\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kyfHUvedlvK_","executionInfo":{"status":"ok","timestamp":1745976508887,"user_tz":360,"elapsed":10,"user":{"displayName":"Matt Elmer","userId":"02352746252018223529"}},"outputId":"2ecc9fcc-8352-46d1-a264-8cd323c65870"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting dataset.py\n"]}]},{"cell_type":"code","source":["%%writefile model.py\n","import torch, torch.nn as nn\n","\n","class EmbroNet(nn.Module):\n","    \"\"\"Tiny CNN encoder + GRU decoder baseline.\"\"\"\n","    def __init__(self, hidden=256):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 32, 3, 2, 1), nn.ReLU(),\n","            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU(),\n","            nn.Conv2d(64,128, 3, 2, 1), nn.ReLU(),\n","            nn.AdaptiveAvgPool2d(1)          # 128√ó1√ó1\n","        )\n","        self.enc_fc  = nn.Linear(128, hidden)\n","\n","        self.gru     = nn.GRU(3, hidden, num_layers=2, batch_first=True)\n","        self.dec_fc  = nn.Linear(hidden, 3)\n","\n","    def forward(self, img, prev_cmds):\n","        B = img.size(0)\n","        h0 = self.encoder(img).view(B, -1)      # B√ó128\n","        h0 = torch.tanh(self.enc_fc(h0)).unsqueeze(0).repeat(2, 1, 1)\n","        out, _ = self.gru(prev_cmds, h0)        # B√óL√óH\n","        return self.dec_fc(out)                 # B√óL√ó3\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAooU3LhlxUf","executionInfo":{"status":"ok","timestamp":1746028053382,"user_tz":360,"elapsed":3,"user":{"displayName":"Matt Elmer","userId":"02352746252018223529"}},"outputId":"c2278c46-ead8-4cfd-b224-37aeb24f88f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing model.py\n"]}]},{"cell_type":"code","source":["import torch\n","import lightning as L\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from dataset import EmbroDataset\n","from model   import EmbroNet\n","\n","# ---------- 1. custom collate_fn ---------------------------------\n","def pad_collate(batch):\n","    \"\"\"\n","    batch = list of (img_tensor, seq_tensor) pairs\n","    Returns:\n","        imgs   : B√ó3√ó128√ó128\n","        tgt    : B√óLmax√ó3  (padded with 0s)\n","        lens   : list[int] (original sequence lengths)\n","    \"\"\"\n","    imgs, seqs = zip(*batch)\n","    imgs = torch.stack(imgs)                   # all same size\n","\n","    lens    = [s.size(0) for s in seqs]\n","    Lmax    = max(lens)\n","    padded  = torch.zeros(len(seqs), Lmax, 3)  # default zeros = padding\n","    for i, s in enumerate(seqs):\n","        padded[i, : lens[i], :] = s\n","    return imgs, padded, torch.tensor(lens)\n","\n","# ---------- 2. LightningModule -----------------------------------\n","class LitModule(L.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = EmbroNet()\n","        self.loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n","\n","    def forward(self, img, prev_cmds):\n","        return self.net(img, prev_cmds)\n","\n","    def training_step(self, batch, _):\n","        img, tgt, lens = batch                 # tgt: B√óLmax√ó3\n","        pred = self(img, tgt[:, :-1])\n","\n","        # compute loss mask-aware\n","        loss = 0.0\n","        for i, L in enumerate(lens):\n","            loss += self.loss_fn(pred[i, : L-1], tgt[i, 1 : L])  # valid region\n","        loss = loss / len(lens)\n","        self.log(\"train_loss\", loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), 1e-3)\n","\n","# ---------- 3. dataset & dataloader ------------------------------\n","tfms = Compose([Resize((128,128)), ToTensor()])\n","ds   = EmbroDataset(IMG_DIR, DST_DIR, tfms)\n","print(\"Dataset length:\", len(ds))          # should be > 0\n","\n","dl   = DataLoader(ds,\n","                  batch_size=8,\n","                  shuffle=True,\n","                  num_workers=0,            # change later if you like\n","                  collate_fn=pad_collate)   # ‚Üê custom collate!\n","\n","# ---------- 4. train ---------------------------------------------\n","trainer = L.Trainer(max_epochs=10,\n","                    precision=\"16-mixed\",\n","                    accelerator=\"auto\")\n","trainer.fit(LitModule(), dl)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":730,"referenced_widgets":["210cb5d424d04ec894bb28b21841bc59","210ef896db1d4bc1957423bd28637835","fd09fec865aa484ca7b3ca13069b6663","969e6f39ccbb4bb39ad712d69ad86694","9d83f7a12e514e7bb564fbca4b8e5c0c","487ecb4c6c27462b9c48e3839d6c72e4","e339ef57905e4c349b06318884c0bb47","98e0d61365c54b8a861b852f9d1fb0e4","142d6798c6f84167be1ca464f11a9c74","a6dcb026b493439995ef961079f9b028","3c18cb04b5034eebb3aa803d9e4212da"]},"id":"uL5UIU_Tlxnz","outputId":"9e716953-84e7-4ff4-e9c7-ab8d57539e78"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n","INFO: Using bfloat16 Automatic Mixed Precision (AMP)\n","INFO:lightning.pytorch.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n","INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO: GPU available: False, used: False\n","INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n","INFO: TPU available: False, using: 0 TPU cores\n","INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO: HPU available: False, using: 0 HPUs\n","INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Dataset length: 162\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: \n","  | Name    | Type     | Params | Mode \n","---------------------------------------------\n","0 | net     | EmbroNet | 722 K  | train\n","1 | loss_fn | MSELoss  | 0      | train\n","---------------------------------------------\n","722 K     Trainable params\n","0         Non-trainable params\n","722 K     Total params\n","2.889     Total estimated model params size (MB)\n","13        Modules in train mode\n","0         Modules in eval mode\n","INFO:lightning.pytorch.callbacks.model_summary:\n","  | Name    | Type     | Params | Mode \n","---------------------------------------------\n","0 | net     | EmbroNet | 722 K  | train\n","1 | loss_fn | MSELoss  | 0      | train\n","---------------------------------------------\n","722 K     Trainable params\n","0         Non-trainable params\n","722 K     Total params\n","2.889     Total estimated model params size (MB)\n","13        Modules in train mode\n","0         Modules in eval mode\n","/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"210cb5d424d04ec894bb28b21841bc59","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","source":["img, tgt = next(iter(dl))\n","net = trainer.model.net.to(img.device)\n","with torch.no_grad():\n","    pred = net(img, tgt[:, :-1])\n","print(\"GT  :\", tgt[0, :5])\n","print(\"Pred:\", pred[0, :5])"],"metadata":{"id":"ZTVlEDlYg75Y"},"execution_count":null,"outputs":[]}]}